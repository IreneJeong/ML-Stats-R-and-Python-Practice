{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning with PyTorch\n\nThroughout this notebook, we use the <a href='https://www.kaggle.com/c/cifar-10/overview'>CIFAR-10</a> dataset from Kaggle, a popular computer-vision dataset of 60,000 32x32 color images to be classified in one of ten classes, with 6000 images per class. This dataset is complex enough to give a good idea of the benefits of fine-tuning, and why this process can be used to achieve high accuracy scores without spending hours and money to re-train complex models from scratch. With **default hyperparameters** and a medium-sized model (ResNet34), we are able to achieve around **96% accuracy** on Kaggle, 3% off the state-of-the-art for this dataset.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n1. [Extracting Data](#extraction)\n2. [Datasets and DataLoaders](#data)  \n3. [Training with Validation](#validation)  \n4. [Full Training](#training)\n5. [Generating Predictions](#testing) ","metadata":{}},{"cell_type":"markdown","source":"## Extracting Data <a name=\"extraction\"></a>\nThe original CIFAR-10 dataset provided by Kaggle is composed of two *.7z* files (*train.7z*, *test.7z*), the labels (*trainLabels.csv*) and an example submission (*sampleSubmission.csv*). We provide these files in the *data* folder.  \n\nFirst, we extract the two zipped files in two folers called *original_train* and *original_test*. Then, we move these files to be in the structure required by the `ImageDataset` class of PyTorch, where every image is stored in a folder named as its label (e.g., the nth airplane image will be stored under *airplane/n.png*). We will create four subfolders within *data*:\n* *train*: contains Training data (excluding Validation set), used to train the model during hyperparameter search\n* *valid*: contains Validation data, used to train the model during hyperparameter search\n* *train_valid*: contains Training+Validation data together, used for full re-training of the model\n* *test* contains Test data, i.e., all the unlabelled data we must submit to Kaggle together with a predicted label","metadata":{}},{"cell_type":"code","source":"# py7zr is required to extract the .7tz files\n\n!pip install -q py7zr","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:11:57.614209Z","iopub.execute_input":"2021-08-06T07:11:57.614678Z","iopub.status.idle":"2021-08-06T07:12:11.336270Z","shell.execute_reply.started":"2021-08-06T07:11:57.614591Z","shell.execute_reply":"2021-08-06T07:12:11.335080Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import py7zr\nimport shutil\nfrom pathlib import Path\n\nroot = Path('./data')\ninput_path = Path('../input/cifar-10')\n\nwith py7zr.SevenZipFile(input_path/'train.7z', mode='r') as z:\n    z.extractall(root)\n\nwith py7zr.SevenZipFile(input_path/'test.7z', mode='r') as z:\n    z.extractall(root)\n\nshutil.copy(input_path/'trainLabels.csv', root/'trainLabels.csv')\n\n(root/'train').rename(root/'original_train')\n(root/'test').rename(root/'original_test')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:12:13.753475Z","iopub.execute_input":"2021-08-06T07:12:13.753920Z","iopub.status.idle":"2021-08-06T07:27:57.839850Z","shell.execute_reply.started":"2021-08-06T07:12:13.753885Z","shell.execute_reply":"2021-08-06T07:27:57.838716Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from random import random\nimport os\n\ndef copy_file(source_directory, destination_directory, filename):\n    \"\"\"\n    Utility function used to copy a file from a source_directory to a destination_directory\n    \"\"\"\n    destination_directory.mkdir(parents=True, exist_ok=True)\n    shutil.copy(source_directory/filename, destination_directory/filename)\n    \ndef organize_train_valid_dataset(root, labels, valid_probability=0.1):\n    \"\"\"\n    Creates the train, train_valid and valid folders respecting PyTorch's ImageDataset structure, performing\n    train/validation split based on the given percentage\n    \"\"\"\n    source_directory = root/'original_train'\n    \n    with os.scandir(source_directory) as it:\n        for entry in it:\n            if entry.is_file():\n                img_index = entry.name.split('.')[0]  # The index is the name of the image except the extension\n                img_class = labels[labels.id==int(img_index)].label.values[0]  # Find the class by looking up the index in the DF\n                \n                # Randomly assign the image to the valid dataset with probability 'valid_probability'\n                channel = Path('train') if random()>valid_probability else Path('valid')\n                destination_directory = root/channel/img_class\n                \n                # Copy the image to either the train or valid folder, and also to the train_valid folder\n                copy_file(source_directory, destination_directory, entry.name)\n                copy_file(source_directory, root/'train_valid'/img_class, entry.name)\n\ndef organize_test_dataset(root):\n    \"\"\"\n    Creates the test folder respecting PyTorch's ImageDataset structure, using a dummy 'undefined' label\n    \"\"\"\n    source_directory = root/'original_test'\n        \n    with os.scandir(source_directory) as it:\n        for entry in it:\n            if entry.is_file():\n                img_index = entry.name.split('.')[0]  # The index is the name of the image except the extension\n\n                channel = Path('test')\n                destination_directory = root/channel/'undefined'\n\n                copy_file(source_directory, destination_directory, entry.name)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:27:57.841773Z","iopub.execute_input":"2021-08-06T07:27:57.842140Z","iopub.status.idle":"2021-08-06T07:27:57.854900Z","shell.execute_reply.started":"2021-08-06T07:27:57.842100Z","shell.execute_reply":"2021-08-06T07:27:57.853493Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read in the labels DataFrame with a label for each image\nlabels = pd.read_csv(root/'trainLabels.csv')\n\n# Create the train/train_valid/valid folder structure\nvalid_probability = 0.1\norganize_train_valid_dataset(root, labels, valid_probability)\n\n# Create the test folder structure\norganize_test_dataset(root)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:27:57.857967Z","iopub.execute_input":"2021-08-06T07:27:57.859103Z","iopub.status.idle":"2021-08-06T07:30:30.997720Z","shell.execute_reply.started":"2021-08-06T07:27:57.859046Z","shell.execute_reply":"2021-08-06T07:30:30.995460Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Datasets and DataLoaders <a name='data'></a>\n\nAs mentioned above, we rely on the `ImageDataset` class of PyTorch to create the required datasets for training, validation, training+validation and testing. Out of each dataset, we then create a DataLoader to be used in the training/evaluation loops to efficiently fetch images in batches from disk.","metadata":{}},{"cell_type":"markdown","source":"We perform an initial step to load in the train data and compute the mean and standard deviation of the dataset for each channel (R, G, B), across all images and all pixels. We compute a mean and stdev value batch-by-batch to avoid loading the entire dataset in memory, and then compute the mean of the means and of the stdevs.  \n**NOTE**: if you have enough RAM (or memory on the GPU), you can use a batch_size equal to the entire train_dataset length, it will provide a more accurate estimation of the means and stdevs by channel.","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade torchvision","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:30:31.008273Z","iopub.execute_input":"2021-08-06T07:30:31.012730Z","iopub.status.idle":"2021-08-06T07:32:14.161735Z","shell.execute_reply.started":"2021-08-06T07:30:31.012664Z","shell.execute_reply":"2021-08-06T07:32:14.160369Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.19.5 which is incompatible.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.9.0 which is incompatible.\nfastai 2.2.7 requires torchvision<0.9,>=0.8, but you have torchvision 0.10.0 which is incompatible.\nallennlp 2.5.0 requires torch<1.9.0,>=1.6.0, but you have torch 1.9.0 which is incompatible.\nallennlp 2.5.0 requires torchvision<0.10.0,>=0.8.1, but you have torchvision 0.10.0 which is incompatible.\u001b[0m\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import torchvision\nimport torch\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    root/'train', \n    transform=torchvision.transforms.Compose([\n        # Resize step is required as we will use a ResNet model, which accepts at leats 224x224 images\n        torchvision.transforms.Resize((224,224)),  \n        torchvision.transforms.ToTensor(),\n    ])\n)\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n\nmeans = []\nstdevs = []\nfor X, _ in train_dataloader:\n    # Dimensions 0,2,3 are respectively the batch, height and width dimensions\n    means.append(X.mean(dim=(0,2,3)))\n    stdevs.append(X.std(dim=(0,2,3)))\n\nmean = torch.stack(means, dim=0).mean(dim=0)\nstdev = torch.stack(stdevs, dim=0).mean(dim=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:32:14.163787Z","iopub.execute_input":"2021-08-06T07:32:14.164382Z","iopub.status.idle":"2021-08-06T07:34:10.442974Z","shell.execute_reply.started":"2021-08-06T07:32:14.164334Z","shell.execute_reply":"2021-08-06T07:34:10.441334Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"The transforms used for the training and training+validation datasets consist of resizing the images to the required resolution by our ResNet model (224x224), using the `AutoAugment` policy learned on the CIFAR10 dataset and finally converting the image from PIL to Tensor. For the validation and test sets we just resize the image and convert it to Tensor format.","metadata":{}},{"cell_type":"code","source":"train_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((224,224)),\n        torchvision.transforms.AutoAugment(policy=torchvision.transforms.AutoAugmentPolicy.CIFAR10),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean, stdev)\n    ])\n\ntrain_dataset, train_valid_dataset = [torchvision.datasets.ImageFolder(folder, transform=train_transforms) for folder in [root/'train', root/'train_valid']]\n\n\nvalid_transforms = torchvision.transforms.Compose([\n        torchvision.transforms.Resize((224,224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize(mean, stdev)\n    ])\n\nvalid_dataset, test_dataset = [torchvision.datasets.ImageFolder(folder, transform=valid_transforms) for folder in [root/'valid', root/'test']]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:34:10.445189Z","iopub.execute_input":"2021-08-06T07:34:10.445657Z","iopub.status.idle":"2021-08-06T07:34:12.886638Z","shell.execute_reply.started":"2021-08-06T07:34:10.445591Z","shell.execute_reply":"2021-08-06T07:34:12.885463Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The train and train+validation DataLoaders use a smaller `batch_size` as we will also need to keep track of gradients in memory. Furthermore, we shuffle the dataset each epoch to avoid loading the batches in the same order.\nThe valid and test DataLoaders use a larger `batch_size` and do not required to shuffle the dataset as we want deterministic results.\n\nThe number of workers is generally set to `2 * num_gpus` as a rule of thumb for Kaggle, with `pin_memory = True` to speed up data transfer to the GPU.","metadata":{}},{"cell_type":"code","source":"num_gpus = torch.cuda.device_count()\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\ntrain_valid_dataloader = torch.utils.data.DataLoader(train_valid_dataset, batch_size=128, shuffle=True, num_workers=2*num_gpus, pin_memory=True)\n\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=2*num_gpus, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:34:12.888525Z","iopub.execute_input":"2021-08-06T07:34:12.888999Z","iopub.status.idle":"2021-08-06T07:34:12.902217Z","shell.execute_reply.started":"2021-08-06T07:34:12.888946Z","shell.execute_reply":"2021-08-06T07:34:12.900316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Training with Validation <a name='validation'></a>\n\nThe first step of the process is to evaluate the model performance on our own Validation set, consisting of 10% of the labelled data we get from Kaggle. Ideally, this step would be performed while finding the best model and hyperparameters to improve the final accuracy. Here, we just perform this step to show the expected model accuracy before submitting the results to Kaggle.  \n**NOTE**: when doing proper hyperparameter tuning, depending on the size of the overall labelle data, a k-fold approach might be more appropriate to estimate the generalization capability of the model.","metadata":{}},{"cell_type":"markdown","source":"We fine-tune a ResNet34 model, trained on ImageNet. Other models might be used, but for the purpose of this notebook a ResNet34 is a good trade-off between training time and model accuracy.  \nThe model originally has a 1000-dimensional output layer, but our dataset has only 10 classes, so we remove the output layer and define a new Fully-Connected layer with just 10 neurons, one for each class in CIFAR-10. The parameters of these new neurons are initialized with Xavier initialization.","metadata":{}},{"cell_type":"code","source":"def get_net():\n    resnet = torchvision.models.resnet34(pretrained=True)\n    \n    # Substitute the FC output layer\n    resnet.fc = torch.nn.Linear(resnet.fc.in_features, 10)\n    torch.nn.init.xavier_uniform_(resnet.fc.weight)\n    return resnet","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:34:12.906359Z","iopub.execute_input":"2021-08-06T07:34:12.907206Z","iopub.status.idle":"2021-08-06T07:34:12.914490Z","shell.execute_reply.started":"2021-08-06T07:34:12.907155Z","shell.execute_reply":"2021-08-06T07:34:12.913141Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The training loop is a standard PyTorch loop where for every epoch we perform the following macro steps:\n1. Iterate over the Train DataLoader by making predictions, calculating loss, backpropagating gradients and updating parameters\n2. Iterate over the Valid DataLoader (if present) to compute the validation loss and accuracy\n3. Decrease the learning rate using the scheduler (if present)\n4. Optionally, store the model checkpoint after a given number of `checkpoint_epochs`","metadata":{}},{"cell_type":"code","source":"import time\n\ndef train(net, train_dataloader, valid_dataloader, criterion, optimizer, scheduler=None, epochs=10, device='cpu', checkpoint_epochs=10):\n    start = time.time()\n    print(f'Training for {epochs} epochs on {device}')\n    \n    for epoch in range(1,epochs+1):\n        print(f\"Epoch {epoch}/{epochs}\")\n        \n        net.train()  # put network in train mode for Dropout and Batch Normalization\n        train_loss = torch.tensor(0., device=device)  # loss and accuracy tensors are on the GPU to avoid data transfers\n        train_accuracy = torch.tensor(0., device=device)\n        for X, y in train_dataloader:\n            X = X.to(device)\n            y = y.to(device)\n            preds = net(X)\n            loss = criterion(preds, y)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                train_loss += loss * train_dataloader.batch_size\n                train_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n        \n        if valid_dataloader is not None:\n            net.eval()  # put network in train mode for Dropout and Batch Normalization\n            valid_loss = torch.tensor(0., device=device)\n            valid_accuracy = torch.tensor(0., device=device)\n            with torch.no_grad():\n                for X, y in valid_dataloader:\n                    X = X.to(device)\n                    y = y.to(device)\n                    preds = net(X)\n                    loss = criterion(preds, y)\n\n                    valid_loss += loss * valid_dataloader.batch_size\n                    valid_accuracy += (torch.argmax(preds, dim=1) == y).sum()\n        \n        if scheduler is not None: \n            scheduler.step()\n            \n        print(f'Training loss: {train_loss/len(train_dataloader.dataset):.2f}')\n        print(f'Training accuracy: {100*train_accuracy/len(train_dataloader.dataset):.2f}')\n        \n        if valid_dataloader is not None:\n            print(f'Valid loss: {valid_loss/len(valid_dataloader.dataset):.2f}')\n            print(f'Valid accuracy: {100*valid_accuracy/len(valid_dataloader.dataset):.2f}')\n        \n        if epoch%checkpoint_epochs==0:\n            torch.save({\n                'epoch': epoch,\n                'state_dict': net.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, './checkpoint.pth.tar')\n        \n        print()\n    \n    end = time.time()\n    print(f'Total training time: {end-start:.1f} seconds')\n    return net","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:34:12.916969Z","iopub.execute_input":"2021-08-06T07:34:12.917457Z","iopub.status.idle":"2021-08-06T07:34:12.938347Z","shell.execute_reply.started":"2021-08-06T07:34:12.917411Z","shell.execute_reply":"2021-08-06T07:34:12.936783Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"In this notebook we only use of at most one GPU, you can freely refactor the code to use DistributedDataParallel if you have more GPUs and/or devices.  \n\nWhen fine-tuning, the model parameters of the network body are trained using a lower learning rate than for the head, since for the latter we have to train them from scratch. We rely on Parameter Groups from PyTorch to define two learning rates for the two groups, and use Adam optimizer with `weight_decay = 5e-4` (find via hyperparameter search).","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nlr, weight_decay, epochs = 1e-5, 5e-4, 20\n\nnet = get_net().to(device)\n\n# Standard CrossEntropy Loss for multi-class classification problems\ncriterion = torch.nn.CrossEntropyLoss()\n\n# params_1x are the parameters of the network body, i.e., of all layers except the FC layers\nparams_1x = [param for name, param in net.named_parameters() if 'fc' not in str(name)]\noptimizer = torch.optim.Adam([{'params':params_1x}, {'params': net.fc.parameters(), 'lr': lr*10}], lr=lr, weight_decay=weight_decay)\n\nnet = train(net, train_dataloader, valid_dataloader, criterion, optimizer, None, epochs, device)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T07:34:12.940100Z","iopub.execute_input":"2021-08-06T07:34:12.940722Z","iopub.status.idle":"2021-08-06T08:27:42.722397Z","shell.execute_reply.started":"2021-08-06T07:34:12.940655Z","shell.execute_reply":"2021-08-06T08:27:42.720883Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545a1bf0b0d74519b58537d1915eb796"}},"metadata":{}},{"name":"stdout","text":"Training for 20 epochs on cuda\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","output_type":"stream"},{"name":"stdout","text":"Training loss: 0.96\nTraining accuracy: 68.24\nValid loss: 0.29\nValid accuracy: 90.35\n\nEpoch 2/20\nTraining loss: 0.41\nTraining accuracy: 85.88\nValid loss: 0.20\nValid accuracy: 93.49\n\nEpoch 3/20\nTraining loss: 0.31\nTraining accuracy: 89.37\nValid loss: 0.17\nValid accuracy: 94.52\n\nEpoch 4/20\nTraining loss: 0.26\nTraining accuracy: 91.15\nValid loss: 0.15\nValid accuracy: 95.05\n\nEpoch 5/20\nTraining loss: 0.23\nTraining accuracy: 92.40\nValid loss: 0.14\nValid accuracy: 95.25\n\nEpoch 6/20\nTraining loss: 0.20\nTraining accuracy: 93.21\nValid loss: 0.13\nValid accuracy: 95.49\n\nEpoch 7/20\nTraining loss: 0.18\nTraining accuracy: 94.09\nValid loss: 0.13\nValid accuracy: 95.79\n\nEpoch 8/20\nTraining loss: 0.16\nTraining accuracy: 94.52\nValid loss: 0.12\nValid accuracy: 95.83\n\nEpoch 9/20\nTraining loss: 0.15\nTraining accuracy: 95.07\nValid loss: 0.12\nValid accuracy: 96.24\n\nEpoch 10/20\nTraining loss: 0.14\nTraining accuracy: 95.37\nValid loss: 0.12\nValid accuracy: 96.16\n\nEpoch 11/20\nTraining loss: 0.13\nTraining accuracy: 95.85\nValid loss: 0.11\nValid accuracy: 96.18\n\nEpoch 12/20\nTraining loss: 0.12\nTraining accuracy: 95.88\nValid loss: 0.11\nValid accuracy: 96.58\n\nEpoch 13/20\nTraining loss: 0.11\nTraining accuracy: 96.32\nValid loss: 0.11\nValid accuracy: 96.30\n\nEpoch 14/20\nTraining loss: 0.11\nTraining accuracy: 96.39\nValid loss: 0.11\nValid accuracy: 96.74\n\nEpoch 15/20\nTraining loss: 0.10\nTraining accuracy: 96.69\nValid loss: 0.10\nValid accuracy: 96.58\n\nEpoch 16/20\nTraining loss: 0.10\nTraining accuracy: 96.75\nValid loss: 0.11\nValid accuracy: 96.62\n\nEpoch 17/20\nTraining loss: 0.09\nTraining accuracy: 96.85\nValid loss: 0.11\nValid accuracy: 96.83\n\nEpoch 18/20\nTraining loss: 0.09\nTraining accuracy: 97.13\nValid loss: 0.11\nValid accuracy: 96.66\n\nEpoch 19/20\nTraining loss: 0.09\nTraining accuracy: 97.22\nValid loss: 0.11\nValid accuracy: 96.62\n\nEpoch 20/20\nTraining loss: 0.08\nTraining accuracy: 97.25\nValid loss: 0.11\nValid accuracy: 96.56\n\nTotal training time: 3208.1 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Full Training <a name='training'></a>\nAfter assessing the model performance on the Validation set, we want to re-train the model on the full Training + Validation data to squeeze every performance left before submitting our results to Kaggle. As a general rule, the more data we train on, the better the results will be.","metadata":{}},{"cell_type":"code","source":"lr, weight_decay, epochs = 1e-5, 5e-4, 20\n\nnet = get_net().to(device)\n\ncriterion = torch.nn.CrossEntropyLoss()\n\nparams_1x = [param for name, param in net.named_parameters() if 'fc' not in str(name)]\noptimizer = torch.optim.Adam([{'params':params_1x}, {'params': net.fc.parameters(), 'lr': lr*10}], lr=lr, weight_decay=weight_decay)\n\nnet = train(net, train_valid_dataloader, None, criterion, optimizer, None, epochs, device)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T08:27:42.728322Z","iopub.execute_input":"2021-08-06T08:27:42.729443Z","iopub.status.idle":"2021-08-06T09:23:21.657572Z","shell.execute_reply.started":"2021-08-06T08:27:42.729385Z","shell.execute_reply":"2021-08-06T09:23:21.656387Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training for 20 epochs on cuda\nEpoch 1/20\nTraining loss: 0.89\nTraining accuracy: 70.41\n\nEpoch 2/20\nTraining loss: 0.38\nTraining accuracy: 86.76\n\nEpoch 3/20\nTraining loss: 0.30\nTraining accuracy: 89.83\n\nEpoch 4/20\nTraining loss: 0.24\nTraining accuracy: 91.69\n\nEpoch 5/20\nTraining loss: 0.21\nTraining accuracy: 92.90\n\nEpoch 6/20\nTraining loss: 0.19\nTraining accuracy: 93.78\n\nEpoch 7/20\nTraining loss: 0.17\nTraining accuracy: 94.38\n\nEpoch 8/20\nTraining loss: 0.15\nTraining accuracy: 94.90\n\nEpoch 9/20\nTraining loss: 0.14\nTraining accuracy: 95.37\n\nEpoch 10/20\nTraining loss: 0.13\nTraining accuracy: 95.78\n\nEpoch 11/20\nTraining loss: 0.12\nTraining accuracy: 95.90\n\nEpoch 12/20\nTraining loss: 0.11\nTraining accuracy: 96.22\n\nEpoch 13/20\nTraining loss: 0.11\nTraining accuracy: 96.41\n\nEpoch 14/20\nTraining loss: 0.10\nTraining accuracy: 96.71\n\nEpoch 15/20\nTraining loss: 0.10\nTraining accuracy: 96.66\n\nEpoch 16/20\nTraining loss: 0.09\nTraining accuracy: 96.85\n\nEpoch 17/20\nTraining loss: 0.09\nTraining accuracy: 96.98\n\nEpoch 18/20\nTraining loss: 0.09\nTraining accuracy: 97.11\n\nEpoch 19/20\nTraining loss: 0.08\nTraining accuracy: 97.22\n\nEpoch 20/20\nTraining loss: 0.08\nTraining accuracy: 97.29\n\nTotal training time: 3338.4 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Generating Predictions <a name='testing'></a>\nAfter re-training the network on the full labelled dataset, we are ready to score the Test set and submit out results to Kaggle. We iterate over the test_dataloader to get a predicted label for each Test image, and then create a final DataFrame like the one provided in *sampleSubmission.csv* to use on Kaggle.","metadata":{}},{"cell_type":"code","source":"preds = []\n\nnet.eval()\nwith torch.no_grad():\n    for X, _ in test_dataloader:\n        X = X.to(device)\n        preds.extend(net(X).argmax(dim=1).type(torch.int32).cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:23:21.659545Z","iopub.execute_input":"2021-08-06T09:23:21.659896Z","iopub.status.idle":"2021-08-06T09:35:38.427546Z","shell.execute_reply.started":"2021-08-06T09:23:21.659859Z","shell.execute_reply":"2021-08-06T09:35:38.426141Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"ids = list(range(1, len(test_dataset)+1))\nids.sort(key=lambda x: str(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:35:38.429600Z","iopub.execute_input":"2021-08-06T09:35:38.430040Z","iopub.status.idle":"2021-08-06T09:35:38.538178Z","shell.execute_reply.started":"2021-08-06T09:35:38.429994Z","shell.execute_reply":"2021-08-06T09:35:38.537075Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'id': ids, 'label': preds})\ndf['label'] = df['label'].apply(lambda x: train_dataset.classes[x])\ndf.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T09:35:38.539717Z","iopub.execute_input":"2021-08-06T09:35:38.540415Z","iopub.status.idle":"2021-08-06T09:35:39.488100Z","shell.execute_reply.started":"2021-08-06T09:35:38.540363Z","shell.execute_reply":"2021-08-06T09:35:39.486889Z"},"trusted":true},"execution_count":15,"outputs":[]}]}