{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 The KerasCV Authors\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"ViT (Vision Transformer) models for Keras.\n",
    "Reference:\n",
    "  - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)\n",
    "    (ICLR 2021)\n",
    "  - [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)\n",
    "    (CoRR 2021)\n",
    "\"\"\"  # noqa: E501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras_cv.layers import TransformerEncoder\n",
    "from keras_cv.layers.vit_layers import PatchingAndEmbedding\n",
    "from keras_cv.models import utils\n",
    "from keras_cv.models.weights import parse_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIGS = {\n",
    "    \"ViTTiny16\": {\n",
    "        \"patch_size\": 16,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 192,\n",
    "        \"mlp_dim\": 768,\n",
    "        \"num_heads\": 3,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTS16\": {\n",
    "        \"patch_size\": 16,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 384,\n",
    "        \"mlp_dim\": 1536,\n",
    "        \"num_heads\": 6,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTB16\": {\n",
    "        \"patch_size\": 16,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 768,\n",
    "        \"mlp_dim\": 3072,\n",
    "        \"num_heads\": 12,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTL16\": {\n",
    "        \"patch_size\": 16,\n",
    "        \"transformer_layer_num\": 24,\n",
    "        \"project_dim\": 1024,\n",
    "        \"mlp_dim\": 4096,\n",
    "        \"num_heads\": 16,\n",
    "        \"mlp_dropout\": 0.1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTH16\": {\n",
    "        \"patch_size\": 16,\n",
    "        \"transformer_layer_num\": 32,\n",
    "        \"project_dim\": 1280,\n",
    "        \"mlp_dim\": 5120,\n",
    "        \"num_heads\": 16,\n",
    "        \"mlp_dropout\": 0.1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTTiny32\": {\n",
    "        \"patch_size\": 32,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 192,\n",
    "        \"mlp_dim\": 768,\n",
    "        \"num_heads\": 3,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTS32\": {\n",
    "        \"patch_size\": 32,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 384,\n",
    "        \"mlp_dim\": 1536,\n",
    "        \"num_heads\": 6,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTB32\": {\n",
    "        \"patch_size\": 32,\n",
    "        \"transformer_layer_num\": 12,\n",
    "        \"project_dim\": 768,\n",
    "        \"mlp_dim\": 3072,\n",
    "        \"num_heads\": 12,\n",
    "        \"mlp_dropout\": 0.0,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTL32\": {\n",
    "        \"patch_size\": 32,\n",
    "        \"transformer_layer_num\": 24,\n",
    "        \"project_dim\": 1024,\n",
    "        \"mlp_dim\": 4096,\n",
    "        \"num_heads\": 16,\n",
    "        \"mlp_dropout\": 0.1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "    \"ViTH32\": {\n",
    "        \"patch_size\": 32,\n",
    "        \"transformer_layer_num\": 32,\n",
    "        \"project_dim\": 1280,\n",
    "        \"mlp_dim\": 5120,\n",
    "        \"num_heads\": 16,\n",
    "        \"mlp_dropout\": 0.1,\n",
    "        \"attention_dropout\": 0.0,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n",
    "    Reference:\n",
    "        - [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929v2)\n",
    "        (ICLR 2021)\n",
    "    This function returns a Keras {name} model.\n",
    "\n",
    "    The naming convention of ViT models follows: ViTSize_Patch-size\n",
    "        (i.e. ViTS16).\n",
    "    The following sizes were released in the original paper:\n",
    "        - S (Small)\n",
    "        - B (Base)\n",
    "        - L (Large)\n",
    "    But subsequent work from the same authors introduced:\n",
    "        - Ti (Tiny)\n",
    "        - H (Huge)\n",
    "\n",
    "    The parameter configurations for all of these sizes, at patch sizes 16 and\n",
    "    32 are made available, following the naming convention laid out above.\n",
    "\n",
    "    For transfer learning use cases, make sure to read the\n",
    "    [guide to transfer learning & fine-tuning](https://keras.io/guides/transfer_learning/).\n",
    "    Args:\n",
    "        include_rescaling: bool, whether to rescale the inputs. If set to\n",
    "            True, inputs will be passed through a `Rescaling(scale=1./255.0)`\n",
    "            layer. Note that ViTs expect an input range of `[0..1]` if rescaling\n",
    "            isn't used. Regardless of whether you supply `[0..1]` or the input\n",
    "            is rescaled to `[0..1]`, the inputs will further be rescaled to\n",
    "            `[-1..1]`.\n",
    "        include_top: bool, whether to include the fully-connected layer at the\n",
    "            top of the network. If provided, num_classes must be provided.\n",
    "        num_classes: optional int, number of classes to classify images into,\n",
    "            only to be specified if `include_top` is True.\n",
    "        weights: one of `None` (random initialization), a pretrained weight file\n",
    "            path, or a reference to pre-trained weights\n",
    "            (e.g. 'imagenet/classification') (see available pre-trained weights\n",
    "            in weights.py). Note that the 'imagenet' weights only work on an\n",
    "            input shape of (224, 224, 3) due to the input shape dependent\n",
    "            patching and flattening logic.\n",
    "        input_shape: optional shape tuple, defaults to (None, None, 3).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be the 4D tensor\n",
    "                output of the last convolutional block.\n",
    "            - `avg` means that global average pooling will be applied to the\n",
    "                output of the last convolutional block, and thus the output of\n",
    "                the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will be applied.\n",
    "            - `token_pooling`, default, means that the token at the start of the\n",
    "                sequences is used instead of regular pooling.\n",
    "        name: (Optional) name to pass to the model, defaults to \"{name}\".\n",
    "        classifier_activation: A `str` or callable. The activation function to\n",
    "            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n",
    "            `classifier_activation=None` to return the logits of the \"top\"\n",
    "            layer.\n",
    "    Returns:\n",
    "      A `keras.Model` instance.\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "@keras.utils.register_keras_serializable(package=\"keras_cv.models\")\n",
    "class ViT(keras.Model):\n",
    "    \"\"\"Instantiates the ViT architecture.\n",
    "\n",
    "    Args:\n",
    "        mlp_dim: the dimensionality of the hidden Dense layer in the transformer\n",
    "            MLP head\n",
    "        include_rescaling: bool, whether to rescale the inputs. If set to\n",
    "            True, inputs will be passed through a `Rescaling(1/255.0)` layer.\n",
    "        name: string, model name.\n",
    "        include_top: bool, whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "            or the path to the weights file to be loaded.\n",
    "        input_shape: optional shape tuple, defaults to (None, None, 3).\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "            - `token_pooling`, default, means that the token at the start of the\n",
    "                sequences is used instead of regular pooling.\n",
    "        num_classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True.\n",
    "                    mlp_dim:\n",
    "        project_dim: the latent dimensionality to be projected into in the\n",
    "            output of each stacked transformer encoder\n",
    "        activation: the activation function to use in the first `layers.Dense`\n",
    "            layer in the MLP head of the transformer encoder\n",
    "        attention_dropout: the dropout rate to apply to the `MultiHeadAttention`\n",
    "            in each transformer encoder\n",
    "        mlp_dropout: the dropout rate to apply between `layers.Dense` layers\n",
    "            in the MLP head of the transformer encoder\n",
    "        num_heads: the number of heads to use in the `MultiHeadAttention` layer\n",
    "            of each transformer encoder\n",
    "        transformer_layer_num: the number of transformer encoder layers to stack\n",
    "            in the Vision Transformer\n",
    "        patch_size: the patch size to be supplied to the Patching layer to turn\n",
    "            input images into a flattened sequence of patches\n",
    "        classifier_activation: A `str` or callable. The activation function to\n",
    "            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n",
    "            `classifier_activation=None` to return the logits of the \"top\"\n",
    "            layer.\n",
    "        **kwargs: Pass-through keyword arguments to `keras.Model`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        weights=None,\n",
    "        input_shape=(None, None, 3),\n",
    "        input_tensor=None,\n",
    "        pooling=None,\n",
    "        num_classes=None,\n",
    "        patch_size=None,\n",
    "        transformer_layer_num=None,\n",
    "        num_heads=None,\n",
    "        mlp_dropout=None,\n",
    "        attention_dropout=None,\n",
    "        activation=None,\n",
    "        project_dim=None,\n",
    "        mlp_dim=None,\n",
    "        classifier_activation=\"softmax\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if weights and not tf.io.gfile.exists(weights):\n",
    "            raise ValueError(\n",
    "                \"The `weights` argument should be either `None` or the path \"\n",
    "                \"to the weights file to be loaded. Weights file not found at \"\n",
    "                \"location: {weights}\"\n",
    "            )\n",
    "\n",
    "        if include_top and not num_classes:\n",
    "            raise ValueError(\n",
    "                \"If `include_top` is True, you should specify `num_classes`. \"\n",
    "                f\"Received: num_classes={num_classes}\"\n",
    "            )\n",
    "\n",
    "        if include_top and pooling:\n",
    "            raise ValueError(\n",
    "                f\"`pooling` must be `None` when `include_top=True`.\"\n",
    "                f\"Received pooling={pooling} and include_top={include_top}. \"\n",
    "            )\n",
    "\n",
    "        inputs = utils.parse_model_inputs(input_shape, input_tensor)\n",
    "        x = inputs\n",
    "\n",
    "        if include_rescaling:\n",
    "            x = layers.Rescaling(1.0 / 255.0, name=\"rescaling\")(x)\n",
    "\n",
    "        # The previous layer rescales [0..255] to [0..1] if applicable\n",
    "        # This one rescales [0..1] to [-1..1] since ViTs expect [-1..1]\n",
    "        x = layers.Rescaling(scale=1.0 / 0.5, offset=-1.0, name=\"rescaling_2\")(\n",
    "            x\n",
    "        )\n",
    "\n",
    "        encoded_patches = PatchingAndEmbedding(project_dim, patch_size)(x)\n",
    "        encoded_patches = layers.Dropout(mlp_dropout)(encoded_patches)\n",
    "\n",
    "        for _ in range(transformer_layer_num):\n",
    "            encoded_patches = TransformerEncoder(\n",
    "                project_dim=project_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_dropout=mlp_dropout,\n",
    "                attention_dropout=attention_dropout,\n",
    "                activation=activation,\n",
    "            )(encoded_patches)\n",
    "\n",
    "        output = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "        if include_top:\n",
    "            output = output[:, 0]\n",
    "            output = layers.Dense(\n",
    "                num_classes, activation=classifier_activation\n",
    "            )(output)\n",
    "\n",
    "        elif pooling == \"token_pooling\":\n",
    "            output = output[:, 0]\n",
    "        elif pooling == \"avg\":\n",
    "            output = layers.GlobalAveragePooling1D()(output)\n",
    "\n",
    "        # Create model.\n",
    "        super().__init__(inputs=inputs, outputs=output, **kwargs)\n",
    "\n",
    "        if weights is not None:\n",
    "            self.load_weights(weights)\n",
    "\n",
    "        self.include_rescaling = include_rescaling\n",
    "        self.include_top = include_top\n",
    "        self.input_tensor = input_tensor\n",
    "        self.pooling = pooling\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.transformer_layer_num = transformer_layer_num\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.activation = activation\n",
    "        self.project_dim = project_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.classifier_activation = classifier_activation\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"include_rescaling\": self.include_rescaling,\n",
    "            \"include_top\": self.include_top,\n",
    "            \"name\": self.name,\n",
    "            \"input_shape\": self.input_shape[1:],\n",
    "            \"input_tensor\": self.input_tensor,\n",
    "            \"pooling\": self.pooling,\n",
    "            \"num_classes\": self.num_classes,\n",
    "            \"patch_size\": self.patch_size,\n",
    "            \"transformer_layer_num\": self.transformer_layer_num,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"mlp_dropout\": self.mlp_dropout,\n",
    "            \"attention_dropout\": self.attention_dropout,\n",
    "            \"activation\": self.activation,\n",
    "            \"project_dim\": self.project_dim,\n",
    "            \"mlp_dim\": self.mlp_dim,\n",
    "            \"classifier_activation\": self.classifier_activation,\n",
    "            \"trainable\": self.trainable,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "def ViTTiny16(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTTiny16\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTTiny16 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vittiny16\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTTiny16\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTTiny16\"][\n",
    "            \"transformer_layer_num\"\n",
    "        ],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTTiny16\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTTiny16\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTTiny16\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTTiny16\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTTiny16\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTS16(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTS16\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTS16 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vits16\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTS16\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTB32\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTS16\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTS16\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTS16\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTS16\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTS16\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTB16(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTB16\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTB16 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vitb16\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTB16\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTB16\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTB16\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTB16\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTB16\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTB16\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTB16\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTL16(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTL16\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTL16 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vitl16\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTL16\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTL16\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTL16\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTL16\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTL16\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTL16\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTL16\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTH16(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTH16\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTH16 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=weights,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTH16\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTH16\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTH16\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTH16\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTH16\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTH16\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTH16\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTTiny32(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTTiny32\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTTiny32 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=weights,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTTiny32\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTTiny32\"][\n",
    "            \"transformer_layer_num\"\n",
    "        ],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTTiny32\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTTiny32\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTTiny32\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTTiny32\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTTiny32\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTS32(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTS32\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTS32 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vits32\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTS32\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTS32\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTS32\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTS32\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTS32\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTS32\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTS32\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTB32(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTB32\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTB32 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=parse_weights(weights, include_top, \"vitb32\"),\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTB32\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTB32\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTB32\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTB32\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTB32\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTB32\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTB32\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTL32(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTL32\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTL32 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=weights,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTL32\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTL32\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTL32\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTL32\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTL32\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTL32\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTL32\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "def ViTH32(\n",
    "    *,\n",
    "    include_rescaling,\n",
    "    include_top,\n",
    "    name=\"ViTH32\",\n",
    "    weights=None,\n",
    "    input_shape=(None, None, 3),\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    num_classes=None,\n",
    "    activation=keras.activations.gelu,\n",
    "    classifier_activation=\"softmax\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Instantiates the ViTH32 architecture.\"\"\"\n",
    "\n",
    "    return ViT(\n",
    "        include_rescaling,\n",
    "        include_top,\n",
    "        name=name,\n",
    "        weights=weights,\n",
    "        input_shape=input_shape,\n",
    "        input_tensor=input_tensor,\n",
    "        pooling=pooling,\n",
    "        num_classes=num_classes,\n",
    "        patch_size=MODEL_CONFIGS[\"ViTH32\"][\"patch_size\"],\n",
    "        transformer_layer_num=MODEL_CONFIGS[\"ViTH32\"][\"transformer_layer_num\"],\n",
    "        project_dim=MODEL_CONFIGS[\"ViTH32\"][\"project_dim\"],\n",
    "        mlp_dim=MODEL_CONFIGS[\"ViTH32\"][\"mlp_dim\"],\n",
    "        num_heads=MODEL_CONFIGS[\"ViTH32\"][\"num_heads\"],\n",
    "        mlp_dropout=MODEL_CONFIGS[\"ViTH32\"][\"mlp_dropout\"],\n",
    "        attention_dropout=MODEL_CONFIGS[\"ViTH32\"][\"attention_dropout\"],\n",
    "        activation=activation,\n",
    "        classifier_activation=classifier_activation,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "setattr(ViTTiny16, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTTiny16\"))\n",
    "setattr(ViTS16, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTS16\"))\n",
    "setattr(ViTB16, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTB16\"))\n",
    "setattr(ViTL16, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTL16\"))\n",
    "setattr(ViTH16, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTH16\"))\n",
    "setattr(ViTTiny32, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTTiny32\"))\n",
    "setattr(ViTS32, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTS32\"))\n",
    "setattr(ViTB32, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTB32\"))\n",
    "setattr(ViTL32, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTL32\"))\n",
    "setattr(ViTH32, \"__doc__\", BASE_DOCSTRING.format(name=\"ViTH32\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
